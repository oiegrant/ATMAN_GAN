from selenium import webdriver
from selenium.webdriver.common.by import By
import requests
import io
import PIL
from PIL import Image
import time

from bs4 import BeautifulSoup
import urllib.request as reqqer



##############################PARAMETER INITIALIZATION#################################

kw_list = ['mountains', 'landscape photograpy', 'deserts','iceland landscapes']
#keyword/phrase list for google searches

count_list = [3,3,3,3]
#number of images to pull for each keyword/phrase


if len(kw_list) != len(count_list):
    print("ERROR: Image count does not equal keyword/phrase count")
    exit()
#exits program if length of count_list and kw_list aren't equal
#once GUI added, will have a default count_list value, say 10 for unassigned categories

#PATH = "/home/oiexx032/Desktop/ATMAN_GAN/chromedriver" #network path
PATH = (r"\Users\grant\OneDrive\Desktop\git_projects\python_envs\ATMAN_env\chromedriver") #home path
#Update PATH to point to local installation of chromedriver


for i in range(len(kw_list)):
    kw_list[i] = kw_list[i].replace(' ', '+')
#reformats kw_list to google-digestible format

wd = webdriver.Chrome(PATH)

##############################CHROME IMAGE SCRAPING FUNCTION############################

def get_images_from_chrome(web_driver, keyword, delay=2, max_images=0):
    #ADD DOCUMENTATION

    def scroll_down(wd):
        #Function for scrolling down web page
        wd.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        #executes a javascript line that scrolls to the bottom of the page
        time.sleep(delay)
        #waiting for images to load before scrolling again


    image_page = (f"https://www.google.com/search?site=&tbm=isch&source=hp&biw=1873&bih=990&q={keyword}")
    wd.get(image_page)
    #goes to google images page of keyword search


    thumb_class = 'Q4LuWd'
    #this is the current image class for the thumbnails (since 9/2021)
    #potential improvement: automate this pull with BeautifulSoup in case google changes these class IDs
    image_class = 'n3VNCb'
    #this is the current image class for the images once clicked onn (since 9/2021) 
    #potential improvement: automate this pull with BeautifulSoup in case google changes these class IDs

    image_urls = set()
    skips = 0

    while len(image_urls) + skips < max_images:
        #ADD DOCUMENTATION
        scroll_down(wd)
        thumbnails = wd.find_elements(By.CLASS_NAME, thumb_class)

        for img in thumbnails[len(image_urls) + skips:max_images]:
            try:
                img.click()
                time.sleep(delay)
            except:
                continue
        
            images = wd.find_elements(By.CLASS_NAME, image_class)
            for image in images:
                if image.get_attribute('src') in image_urls:
                    max_images += 1
                    skips += 1
                    break
                if image.get_attribute('src') and 'http' in image.get_attribute('src'):
                    image_urls.add(image.get_attribute('src'))
                    
    return image_urls

########################################################################################

##############################DOWNLOAD IMAGES VIA LINK TO DEVICE########################

def download_image(download_path, url, file_name):
    #ADD DOCUMENTATION
    try:
        image_content = requests.get(url).content
        #saves as binary
        image_file = io.BytesIO(image_content)
        #saves as image
        image = Image.open(image_file)
        #saving to location...
        file_path = download_path + file_name

        with open(file_path, "wb") as f:
            image.save(f, "JPEG")
    except Exception as e:
        print('FAILED---', e)

########################################################################################

##############################PROGRAM RUN###############################################
#NOTE: Must have images folder created in your local folder, I use imgs/ here

for i, kw in enumerate(kw_list):
    temp_url_list = get_images_from_chrome(wd,kw,max_images = count_list[i])
    for i, url in enumerate(temp_url_list):
        download_image("imgs/", url, f'{kw}_{str(i)}.jpg')

wd.quit()

########################################################################################